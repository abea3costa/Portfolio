Hi ![](https://user-images.githubusercontent.com/18350557/176309783-0785949b-9127-417c-8b55-ab5a4333674e.gif)My name is Beatriz Costa
=====================================================================================================================================

Data Analyst
------------

### About
I hold a Bachelor of Science degree with a strong foundation in mathematics and statistics, and I‚Äôm about to graduate from the Just IT Data Technician Bootcamp. I transitioned from clinical work into data analysis to explore my enthusiasm for identifying patterns and trends. During my journey, I‚Äôve honed my technical skills using tools like MySQL, Excel, Power BI, Tableau, Python, and R. My goal is to leverage my skills to help organizations make data-driven decisions and promote business growth. Looking for new opportunities to grow professionally and make meaningful contributions as a Data Analyst. I‚Äôm excited to apply my experience with BI tools and my problem-solving mindset to real-world data challenges. I‚Äôm always seeking new opportunities for personal and professional development, and I'm eager to collaborate on exciting projects!

*   üåç¬† I'm based in Newcastle Upon Tyne
*   üß†¬† I'm learning the best ways to showcase my skills
*   ü§ù¬† I'm open to collaborating on exciting data-driven projects

### Skills
<p align="left">
<a href="https://www.python.org/" target="_blank" rel="noreferrer"><img src="https://raw.githubusercontent.com/danielcranney/readme-generator/main/public/icons/skills/python-colored.svg" width="36" height="36" alt="Python" /></a><a href="https://www.r-project.org/" target="_blank" rel="noreferrer"><img src="https://raw.githubusercontent.com/danielcranney/readme-generator/main/public/icons/skills/rlang-colored.svg" width="36" height="36" alt="rlang" /></a><a href="https://git-scm.com/" target="_blank" rel="noreferrer"><img src="https://raw.githubusercontent.com/danielcranney/readme-generator/main/public/icons/skills/git-colored.svg" width="36" height="36" alt="Git" /></a><a href="https://www.mysql.com/" target="_blank" rel="noreferrer"><img src="https://raw.githubusercontent.com/danielcranney/readme-generator/main/public/icons/skills/mysql-colored.svg" width="36" height="36" alt="MySQL" /></a><a href="https://www.adobe.com/uk/products/photoshop.html" target="_blank" rel="noreferrer"><img src="https://raw.githubusercontent.com/danielcranney/readme-generator/main/public/icons/skills/photoshop-colored.svg" width="36" height="36" alt="Photoshop" /></a><a href="https://cloud.google.com/" target="_blank" rel="noreferrer"><img src="https://raw.githubusercontent.com/danielcranney/readme-generator/main/public/icons/skills/googlecloud-colored.svg" width="36" height="36" alt="Google Cloud" /></a>
                    </p>

### Socials

<p align="left"> <a href="https://www.github.com/abea3costa" target="_blank" rel="noreferrer"> <picture> <source media="(prefers-color-scheme: dark)" srcset="https://raw.githubusercontent.com/danielcranney/readme-generator/main/public/icons/socials/github-dark.svg" /> <source media="(prefers-color-scheme: light)" srcset="https://raw.githubusercontent.com/danielcranney/readme-generator/main/public/icons/socials/github.svg" /> <img src="https://raw.githubusercontent.com/danielcranney/readme-generator/main/public/icons/socials/github.svg" width="32" height="32" /> </picture> </a></p>
                    
### CV [pdf](https://github.com/abea3costa/Portfolio/blob/main/CVBCpublic.pdf)


## Table of Contents
- [About](https://github.com/abea3costa/Portfolio/blob/main/README.md#about)
- [Portfolio Projects](https://github.com/abea3costa/Portfolio/blob/main/README.md#portfolio-projects)
  - Python
    - [GDP (nominal) per Capita](https://github.com/abea3costa/Portfolio/blob/main/README.md#gdp-nominal-per-capita)
    - [BMI Calculator in Python (Jupyter Notebook)](https://github.com/abea3costa/Portfolio/blob/main/README.md#bmi-calculator-in-python-jupyter-notebook)
  - MySQL
    - [Creating a Database]()
    - [Data Cleaning](https://github.com/abea3costa/Portfolio/blob/main/README.md#data-cleaning)
    - [Tech Layoffs - Exploratory Data Analysis](https://github.com/abea3costa/Portfolio/blob/main/README.md#tech-layoffs---exploratory-data-analysis)
  - Excel
    - [Coffee Sales Dashboard](https://github.com/abea3costa/Portfolio/blob/main/README.md#coffee-sales-dashboard)
    - [Bike Sales Dashboard](https://github.com/abea3costa/Portfolio/blob/main/README.md#bike-sales-dashboard)
  - Tableau
    - [WHO Health Survey](https://github.com/abea3costa/Portfolio/blob/main/README.md#who-health-survey)
    - [Spotify Listeners Analysis](https://github.com/abea3costa/Portfolio/blob/main/README.md#spotify-listeners-analysis)
    - [Gaming Sales](https://github.com/abea3costa/Portfolio/blob/main/README.md#gaming-sales)
  - Power BI
    - [Tech Job Satisfaction Survey](https://github.com/abea3costa/Portfolio/blob/main/README.md#tech-job-satisfaction-survey)
    - [Apocalypse Preparation Analysis](https://github.com/abea3costa/Portfolio/blob/main/README.md#apocalypse-preparation-analysis)
  - Azure
    - [Paws & Whiskers transition to Microsoft Azure](https://github.com/abea3costa/Portfolio/blob/main/README.md#paws--whiskers-transition-to-microsoft-azure)


## Portfolio Projects

This repository contains a collection of projects which showcase my work and skills as a Data Analyst. 



## Python

## GDP (nominal) per Capita

### Overview
This project aims to analyse how different estimates relate to each other. The dataset contains information on UN, World Bank and IMF GDP estimates across different countries/territories in the world.

### Technologies Used
- Python (pandas, matplotlib, seaborn)

### Dataset Description
- The dataset consists of 224 records featuring the different GDP estimates and the year these were recorded.
- Source: Kaggle's dataset: [GDP (in USD) Per Capita Income by Country](https://www.kaggle.com/datasets/rajkumarpandey02/gdp-in-usd-per-capita-income-by-country/data) 

### Methodology
- Data Cleaning: Handled missing values using averages and removed outliers.
- Exploratory Data Analysis: Calculated average estimates and visualised if there was a positive or negative correlation between them.
- Analysis: Verified how well each continent was represented by counting the number of records (Countries) per continent. Calculated the average IMF Estimate per continent. Calculated the World Bank Estimate by Year and Continent.
- Visualisation: Built correlation matrix between estimates. Created visualisations for the analysis above.

### Visualisations
- Correlation Matrix between estimates
![Correlation matrix between estimates](https://github.com/abea3costa/Portfolio/blob/main/Images%20GDP%20python%20project/Correlation%20matrix%20between%20estimates.png)
- Continents by IMF Estimate
![Continents by IMF Estimate](https://github.com/abea3costa/Portfolio/blob/main/Images%20GDP%20python%20project/Continents%20by%20IMF%20Estimate.png )
- World Bank Estimates by Year and Continent
![World Bank Estimates by Year and Region](https://github.com/abea3costa/Portfolio/blob/main/Images%20GDP%20python%20project/World%20Bank%20Estimates%20by%20Year%20and%20Region.png)

### Key Findings
- The GDP estimates from the World Bank and the UN showed a strong positive correlation.
- The World Bank and the IMF GDP estimates, as well as the UN and IMF GDP estimates, also show a positive correlation, although not as strong as the one observed between the World Bank and UN estimates.
- Each continent was represented by a comparable sample of approximately 50 country records, with the exception of Oceania‚Äôs sample, which only included 20 country records.
- Europe holds the highest GDP estimates and represents the highest rates of record keeping over the years when compared to other continents which present gaps in GDP estimate data.

### Results & Conclusions
- The analysis recommends that the records be dated and organized by year to prevent the risk of comparing the three distinct GDP estimates from different years.
- To ensure a more accurate and streamlined analysis, certain issues related to data collection need to be addressed. These issues include the substantial amount of missing data and the inadequate recording of the years in which each estimate had been recorded, within the same row.

### Challenges & Limitations
- 11% of the cells in this dataset contained missing values, which is fairly substantial. Although these values were replaced with average estimates, this likely impacted the accuracy of the analysis.



## BMI Calculator in Python (Jupyter Notebook)

### Overview
This project‚Äôs aim is to create a BMI calculator based on the NHS formula and classification criteria. The BMI calculator is a simple tool that calculates a person's BMI based on their weight and height. The BMI value is then categorized into different ranges (Underweight, Normal weight, Overweight, Obese and Severely Obese) to give users an indication of their health status.

### Technologies Used
- Python: The core programming language for the BMI calculation logic.
- Jupyter Notebook: Used for an interactive coding experience, making it easy to showcase the program and output.

### Dataset Description
- [BMI Formula Source]( https://www.nhs.uk/health-assessment-tools/calculate-your-body-mass-index/calculate-bmi-for-adults)
- [BMI classification source]( https://www.nhsinform.scot/healthy-living/food-and-nutrition/healthy-eating-and-weight-management/body-mass-index-bmi/)

### Key Features
- User Input: The program prompts the user to enter their weight (in kilograms) and height (in meters).
- BMI Calculation: The formula used to calculate BMI is:
BMI=(weight/((height*0.01)2))
- BMI Classification: The program outputs the calculated BMI along with a classification based on the value:
  - Underweight: BMI < 18.5
  - Normal weight: 18.5 ‚â§ BMI < 24.9
  - Overweight: 25 ‚â§ BMI < 29.9
  - Obese: 30 ‚â§ BMI < 39.9
  - Severely obese: BMI ‚â• 40

### How to Run the Code
1.	Clone the repository: git clone [Repository](https://github.com/abea3costa/Portfolio)
2.	Navigate to the project directory: cd [BMI Calculator in Python (Jupyter Notebook)](https://github.com/abea3costa/Portfolio/blob/main/README.md#bmi-calculator-in-python-jupyter-notebook)
3.	Open the Jupyter Notebook: [BMI calculator]( https://github.com/abea3costa/Portfolio/blob/main/BMI%20calculator.ipynb)
4.	Run the notebook and follow the instructions for input.

### Future Work
- Matplotlib: Visualization tools, like creating plots to show BMI categories or trends over time (if extended functionality is implemented).
- Pandas: Can be used for collecting, storing, or analysing data, such as BMI data of multiple users.



# MySQL

# Creating a Database

## Overview
The goal of this essay is to outline the steps I would take to set up and create a database for a small retail business. Throughout this essay, I will provide examples of SQL commands and explain the rationale behind each step, highlighting why they are essential for creating a functional and efficient database tailored to the needs of the retail business.

## Context
_Imagine you have been hired by a small retail business that wants to streamline its operations by creating a new database system. This database will be used to manage inventory, sales, and customer information. The business is a small corner shop that sells a range of groceries and domestic products. It might help to picture your local convenience store and think of what they sell. They also have a loyalty program, which you will need to consider when deciding what tables to create._

## Setting Up and Creating a Database for a Small Retail Business: A Step-by-Step Guide
To help a small retail business selling groceries and domestic products, I aim to create a structured and efficient database system to manage inventory, sales, customer data, and a loyalty program. The system will serve as a centralized repository, storing product details, sales transactions, customer profiles, and loyalty points, improving the management of daily business operations. The primary users of the database will include store employees, managers, and administrators, each with specific responsibilities. Store staff will enter and track sales and inventory. Managers will use the database to generate reports and monitor sales performance, inventory levels, and customer information. Administrators will handle database security and role-based access control to ensure data integrity and privacy. 
The first step in designing the database is creating an Entity Relationship Diagram (ERD). This diagram will include key tables such as Products, Sales, Customers, and Date. The Products table will store information like product names, descriptions, and prices. The Sales table will record transactions, linking products sold to customers and tracking quantities and total amounts. The Customers table will store customer information, including contact details and loyalty points, while the Date table will capture time-related dimensions such as months, quarters, and days of the week. 
The database schema (illustrated below) will feature the Sales table as the central fact table, which contains transactional metrics such as total sales, quantities sold, and foreign keys linking it to dimension tables. The Products, Customers, and Date tables will be dimension tables that provide descriptive attributes for sales analysis. The Date table will be essential for time-based analysis, enabling the business to evaluate sales trends over specific periods like months, quarters or years. For database implementation, mySQL commands such as ‚ÄúCREATE DATABASE,‚Äù ‚ÄúCREATE TABLE,‚Äù and ‚ÄúALTER TABLE‚Äù will be used to define the structure, establish relationships, and set up tables. For instance, the following mySQL commands would create the database and the Customer_dim table: ‚ÄúCREATE DATABASE retailDB; CREATE TABLE Customer_dim (CustomerID INT PRIMARY KEY, First_name VARCHAR(30), Last_name VARCHAR(30));‚Äù.
![schema]()
After defining the tables, I would populate them with initial data using INSERT INTO commands. For example: ‚ÄúINSERT INTO Customer_dim (CustomerID, First_name, Last_name) VALUES (1, ‚ÄúJohn‚Äù, ‚ÄúSmith‚Äù);‚Äù. 
To maintain the database, I would implement regular updates to inventory records and customer profiles as sales are processed. I would use data integrity constraints such as primary keys, foreign keys, and check constraints to ensure data consistency and accuracy. Additionally, I would set up scheduled backups to safeguard against data loss and ensure business continuity in the event of system failures. Security measures, such as role-based access control, will be implemented to limit access to sensitive data and define permissions based on user roles. 

## Conclusion
This database system will serve as the foundation of the retail business, facilitating effective operations, accurate reporting, and efficient management of inventory, sales, and customer data. By implementing best practices in data integrity, security, and backup strategies, the system will guarantee the business stays reliable, adaptable, and robust in the long run.


# Data Cleaning

## Overview
In this project my aim is to clean the dataset using MySQL. I will start by removing supplicates, standardising the data, handling missing/NULL values and finally deleting irrelevant columns.

## Technologies Used
- MySQL Workbench (version 8.0.38)

## Dataset Description
- The dataset consists of 2361 records featuring the various tech companies‚Äô layoff and redundancies data including columns such as industry, country, location, percentage layoff and total layoff. 
- Source: AlexTheAnalyst GitHub: [Company layoffs]( https://github.com/AlexTheAnalyst/MySQL-YouTube-Series/blob/main/layoffs.csv) 

## Methodology
1.	Created a copy of the dataset to keep the original intact.
![ss1]( https://github.com/abea3costa/Portfolio/blob/main/MySQL%20SS/Data%20Cleaning%201.png)

2.	Removed duplicates using a CTE (Common Table Expressions) and creating row number column. This method allows me to identify unique rows (row number=1) and duplicates (row number>1).
![ss2]( https://github.com/abea3costa/Portfolio/blob/main/MySQL%20SS/Removing%20duplicates.png)
![ss3]( https://github.com/abea3costa/Portfolio/blob/main/MySQL%20SS/Removing%20duplicates%202.png)

3.	Standardised data ‚Äì using the TRIM() function and converted the dates from string to date data types, in a uniform format.

- TRIM() function
![ss4]( https://github.com/abea3costa/Portfolio/blob/main/MySQL%20SS/Standerdising%20data%20After.png)
![ss5]( https://github.com/abea3costa/Portfolio/blob/main/MySQL%20SS/Standerdising%20data%20After%202.png)

- Data type conversion (String to Date)
![ss6]( https://github.com/abea3costa/Portfolio/blob/main/MySQL%20SS/Standerdising%20data%20Date%20format%20After.png)

4.	Handled missing values and/or NULL values ‚Äì used an INNER JOIN to fill in missing and NULL values for industry, utilising data from other rows belonging to the same company and location.
![ss7]( https://github.com/abea3costa/Portfolio/blob/main/MySQL%20SS/Standerdising%20Join%20to%20populate%20industry.%20png.png)

5.	There was a significant number of rows which had no data for total_laid_off and percentage_laid_off. I proceeded to delete these as they would make the analysis less accurate and more difficult. I also deleted irrelevant columns.
![ss8]( https://github.com/abea3costa/Portfolio/blob/main/MySQL%20SS/Null%20values%20After.png)


## Key Findings
- Clean data leads to more accurate and reliable analysis results, enabling better decision-making. 
- By removing noise and errors, data cleaning helps to reveal meaningful patterns and insights. 
- Improving data quality can reduce costs associated with data storage, processing, and analysis. 
- Clean data can lead to better business outcomes by enabling more informed and data-driven decisions. 

## Results & Conclusions
- This project allowed me to enhance and strengthen my data cleaning skills with MySQL. Data cleaning is such a crucial part of any data analysis or database management process, and MySQL offers powerful features such as UPDATE, DELETE, REPLACE, and the use of JOIN and CASE statements which make for an efficient data cleaning process.

## Challenges & Limitations
- I encountered some issues when running the INNER JOIN in order to find and delete the NULL and missing values. However, after seeking further material and reviewing the Just IT class materials on Joins in MySQL, I managed to correct the simple syntax problem I had.



## Tech Layoffs - Exploratory Data Analysis

## Overview
In this project I will be performing some exploratory data analysis in order to obtain some insights into tech companies‚Äô layoffs between November 2020 and June 2023.

## Technologies Used
- MySQL Workbench (version 8.0.38)

## Dataset Description
- The dataset consists of 2361 records featuring the various tech companies‚Äô layoff and redundancies data including columns such as industry, country, location, percentage layoff and total layoff. 
- Source: AlexTheAnalyst GitHub: [Company layoffs]( https://github.com/AlexTheAnalyst/MySQL-YouTube-Series/blob/main/layoffs.csv) 

## Methodology
- Started by having a look at the data overall using the SELECT statement. 
- Queried the maximum total_laid_off and maximum percentage_laid_off utilising the SELECT and MAX statements. 
- Used RENAME to change the percentage_laid_off column name to decimal_percent_layoff as the values aren‚Äôt presented as percentages, but as decimal/proportional representations of percentages (e.g. 1 instead of 100%).
- Proceeded to filter the data with the WHERE statement and sorted the total_laid_off data from highest to lowest with ORDER BY‚Ä¶ DESC.
![EDA1]( https://github.com/abea3costa/Portfolio/blob/main/Tech%20layoffs%20analysis/EDA%201%20-%20max%20and%20rename.png)
- Applied the same technique, but used ORDER BY‚Ä¶ DESC on funds_raised_millions find out how much money had been obtained making employees redundant.
![EDA2]( https://github.com/abea3costa/Portfolio/blob/main/Tech%20layoffs%20analysis/EDA%202%20-%20frm.png)
- Confirmed the exact dates this dataset was referring to using MIN(‚Äòdate‚Äô) and MAX(‚Äòdate‚Äô)
- Calculated the companies who fired the most employees using the SUM(total_laid_off), GROUP BY company and sort by the total layoffs using ORDER BY ‚Ä¶ DESC.
![EDA3]( https://github.com/abea3costa/Portfolio/blob/main/Tech%20layoffs%20analysis/EDA%203%20-%20who%20fired%20the%20most.png)
- A CTE allowed me to count the total number of companies that fired all their employees.
![EDA3.1]( https://github.com/abea3costa/Portfolio/blob/main/Tech%20layoffs%20analysis/EDA%203%20-%20how%20many%20fired%20100percent.png)
- I discovered which industries had the most layoffs and identified the countries that experienced the highest number of layoffs.
![EDA4]( https://github.com/abea3costa/Portfolio/blob/main/Tech%20layoffs%20analysis/EDA%204%20-%20who%20fired%20the%20most-industry.png)
![EDA5]( https://github.com/abea3costa/Portfolio/blob/main/Tech%20layoffs%20analysis/EDA%205%20-%20who%20fired%20the%20most-country.png)
- Found out which year had the most layoffs using YEAR(‚Äòdate‚Äô), SUM(total_laid_off), GROUP BY and ORDER BY. There were a few NULL values on the year, so I proceeded to not include them in my result using the WHERE clause.
![EDA6]( https://github.com/abea3costa/Portfolio/blob/main/Tech%20layoffs%20analysis/EDA%206-%20yearly%20layoffs.png)
- Used the AVG function and ROUND to calculate the average total layoffs by company.
![EDA7]( https://github.com/abea3costa/Portfolio/blob/main/Tech%20layoffs%20analysis/EDA%207%20-%20average%20layoffs.png)
- I created a substring to extract the year and month from the full date and then sorted the data by year using ORDER BY. Next, I used the substring logic within a set of statements to build a CTE, which allowed me to calculate the rolling total of layoffs. 
![EDA8]( https://github.com/abea3costa/Portfolio/blob/main/Tech%20layoffs%20analysis/EDA%208%20-%20rolling%20total.png)
- I used the SUM and ROUND functions on the decimal_percent_layoff, grouped the data by company, and sorted it in descending order (SORT BY...DESC) to identify any companies that might have fired all their employees twice during the registered period.
![EDA9]( https://github.com/abea3costa/Portfolio/blob/main/Tech%20layoffs%20analysis/EDA%209%20-%20companies%20that%20fired%20all%20employes%20more%20than%20once.png)
- Created a CTE to find out the top 5 companies with most layoffs sorted by year using DENSE_RANK() OVER(PARTITION BY‚Ä¶ORDER BY‚Ä¶DESC)
![EDA10]( https://github.com/abea3costa/Portfolio/blob/main/Tech%20layoffs%20analysis/EDA%2010%20-%20top%205%20by%20year.png)
- Created a CTE to find out the top months (and associated year) the maximum layoffs had been registered. 
![EDA11]( https://github.com/abea3costa/Portfolio/blob/main/Tech%20layoffs%20analysis/EDA%2011%20-%20top%20month%20max%20layoffs.png)
- Created another CTE to rank the month (and associated year) with the highest total_laid_off.
![EDA12]( https://github.com/abea3costa/Portfolio/blob/main/Tech%20layoffs%20analysis/EDA%2012-%20last%20CTE.png)

## Key Findings
- This dataset covers the period from November 3, 2020, to June 3, 2023, during which a total of 385,879 employees were laid off from tech companies.
- The year 2022 was marked by the highest number of layoffs for the registered period with a total of 161711 employees.
- 116 companies fired all their employees.
- The top 5 companies that fired the most employees were Amazon, Google, Meta, Salesforce and Phillips. Google held the highest average layoffs, followed by Meta and Microsoft. 
- The top 5 industries with most employees fired were consumer, retail, other, transportation and finance. 
- The top 5 countries with most layoffs registered were the United States, India, the Netherlands, Sweden and Brazil.
- Here are the top 5 companies with most layoffs per year in order of most layoffs:
	- 2020: Uber, Booking.com, Groupon, Swiggy and Airbnb
	- 2021: Bytedance, Katerra, Zillow, Instacart and WhiteHat Jr.
	- 2022: Meta, Amazon, Cisco, Peloton, Carvana and Phillips (sharing the 5th place)
	- 2023: Google, Microsoft, Ericsson, Amazon and Salesforce (sharing the 4th place) and Dell. 
- The month with the highest number of layoffs was April 2020, with 26,710 employees being fired. Following April, the months with the most layoffs were May, March, June, and July of 2020, completing the top 5 months with the highest number of layoffs registered. 

## Results & Conclusions
- This project was a great opportunity for me to assess my understanding of Structured Query Language (SQL). It highlighted certain areas, like CTEs, that required more focus. After seeking out additional resources to practice CTEs, I became much more confident and improved my skills with them. Completing this project helped me solidify my SQL knowledge, and I now feel more confident in handling similar challenges with greater ease moving forward.

## Challenges & Limitations
- It seemed as though a company named Service had laid off all its employees on two separate occasions between November 2020 and June 2023. However, the 100% layoffs were actually logged twice, just four days apart‚Äîonce on March 16, 2020, and again on March 20, 2020. It's likely that these two records refer to the same event, recorded twice.
- Some of the CTEs were quite complex to write, so I practiced using the same dataset and delved into additional materials and real-life exercises to continue improving my skills in writing CTEs at the same level of difficulty.


# Excel

## Coffee Sales Dashboard

![Coffee Sales Dashboard](https://github.com/abea3costa/Portfolio/blob/main/Excel%20Dashboards/Coffee_Sales_Dashboard.png)

## Overview
This project‚Äôs objective is to create an intuitive dashboard including the key insights of sales analysis in a coffee shop such as revenue, payment mode and most popular and most revenue generating coffee orders.

## Technologies Used
- Excel: EDA formulas such as MIN(), MAX(), AVERAGE(), COUNT() to describe sales and coffee prices, formulas such as SUMIF() and COUNTIF() to group total revenue by coffee order, pivot tables to summarise the data and charts to visually display findings.

## Dataset Description
- The dataset consists of 2838 records containing details of customer orders like product ordered, price and payment method.
- Source: Kaggle's dataset: [Coffee Sales]( https://www.kaggle.com/datasets/ihelon/coffee-sales) 

## Methodology
- Data Cleaning: Ensured the data was standardized, verified that data types were in the correct format, and removed any duplicates.
- Exploratory Data Analysis: Calculated minimum, maximum and average brew prices as well as calculated the total recorded sales transactions.
- Analysis: Calculated the coffee shop order popularity, coffee orders generating the most revenue total orders over time. 
- Visualisation: The dashboard utilizes charts and graphs to visually represent key insights which resulted from my analysis, making it easier to identify trends and patterns.

## Key Findings
- Revenue and total orders are at their peak in October with a second less significant peak in May. The lowest registered sales were in January and April.
- The card is most evidently the most popular payment method.
- The most popular orders are the Americano with Milk and the Latte, while the least popular are the espresso and cocoa. 
- The Latte generates the majority of the revenue, and while the Americano with milk is ordered slightly more frequently, its lower price results in less revenue compared to the Latte.

## Results & Conclusions
- To boost coffee sales in January, I recommend introducing coffee shop vouchers that customers can give as Christmas gifts to family and friends.
- Based on my research, the decline in sales in April may be linked to a drop in monthly export volume, resulting from a smaller coffee bean harvest in certain regions. This can be addressed by offering alternatives to coffee, such as breakfast teas or other tea varieties, or by exploring other coffee supplier options.

## Challenges & Limitations
- The dataset lacked information on the types of coffee beans and the suppliers, which would have been helpful in identifying the cause of the reduced revenue in April and exploring more targeted alternatives to address the issue.



## Bike Sales Dashboard

![Bike Sales](https://github.com/abea3costa/Portfolio/blob/main/Excel%20Dashboards/Bike%20Sales.png)

## Overview
The objective of this project is to create an intuitive dashboard with sliders that highlights key insights from bike sales analysis and demographics. I examine how factors such as region, commuting distance, education level, income, gender, and age influence customers' bike purchases.

## Technologies Used
- Excel: EDA formulas such as MIN(), MAX(), AVERAGE(), COUNT() to describe bike sales, IF statements to categorise customers age in groups (Adolescent, Middle Age and Old), pivot tables to summarise the data and charts and sliders to visually display findings in an intuitive dashboard.

## Dataset Description
- The dataset consists of 1000 records containing details of customer demographics such as age, gender, marital status, level of education and income.
- Source: AlexTheAnalyst GitHub: [Bike Sales](https://github.com/AlexTheAnalyst/Excel-Tutorial/blob/main/Excel%20Project%20Dataset.xlsx) 

## Methodology
- Data Cleaning: Ensured the data was standardized, verified that data types were in the correct format, and removed any duplicates as well as any irrelevant columns.
- Exploratory Data Analysis: using pivot tables, summarised and explored bike purchases by customer demographics.
- Analysis: Calculated average income and grouped customer data by gender to compare income and gender to bike purchases. Analysed bike purchases by age group and examined how commute distance to work influenced bike sales.
- Visualisation: The dashboard features charts and graphs to visually display key insights derived from my analysis, making it easier to identify customer buying patterns. It also includes slicers to filter data based on additional factors such as marital status, region, education, and homeownership (Yes/No).

## Key Findings
- Based on the available sample, the customers who purchased the most bikes are males, middle aged customers, from the North American region and with commutes up to 1 mile. Customers with a bachelor‚Äôs degree, higher average incomes and home owners also buy more bikes.  

## Results & Conclusions
- To increase bike sales in other age groups, discounts can be offered to adolescents and the elderly, as they typically have lower incomes compared to middle aged people.
- To boost sales among women, marketing strategies can focus on designing bikes that are more comfortable and visually appealing to them. For instance, conducting a survey to understand what women look for when purchasing a bike (e.g. colour, seat type, attached basket, etc.) could provide valuable insights.

## Challenges & Limitations
- The dataset contained a good quality sample of the bike shop customers and it allowed me to explore how a variety of factors affected bike sales. 



# Tableau

## WHO Health Survey

![WHO Health Survey](https://github.com/abea3costa/Portfolio/blob/main/Tableau%20Dashboards/WHO%20Cancer%20Data%201990-2008.png)

## Overview
The objective of this project is to create an intuitive dashboard which highlights key insights from the World Health Organisation (WHO) cancer rates data between the years of 1990 and 2008.

## Technologies Used
- Tableau: Used different sheets for each visualisation to then combine them in a dashboard. Utilised calculated fields for cancer incidence rates around the world (Incidence Rate= (Number of new cases) / (Total population) * 100).

## Dataset Description
- The dataset consists of 6004 records containing details of the world population liver, stomach and lung cancer rates as well as different risk factors.
- Source: JustIT Data Technician Bootcamp: [World Health Organisation (WHO) cancer rates 1990-2008](Day_2_Task_2_Health_Survey.twbx)

## Methodology
- Visualisation: The dashboard contains a heatmap, a table, a pie chart, colunm and bar charts, to visually represent key insights regarding cancer rates across the globe, making it easier to identify a growing trend over time and risk factor patterns.
- User-Friendly Interface: The dashboard is designed with a user-friendly interface, ensuring easy navigation and accessibility.
- Customizable: The dashboard is highly customizable, allowing users to add or modify charts and tables based on specific changes in data.

## Key Findings
- Cancer rates are notably high in Asia, Europe, and the Americas.
- The cancer rates worldwide have been increasing over time.
- Men have a higher incidence of stomach, lung, and liver cancers compared to women.
- For the cancers where data is available (liver, lung, and stomach), factors such as a high BMI or high blood pressure do not appear to significantly influence the volume of cases. However, high cholesterol levels seem to be associated with a higher volume of cases for these types of cancers.

## Results & Conclusions
- National Health Systems can use the increasing global cancer rates as a call to action to enhance prevention, early detection, treatment, and support services.
- Public health education, particularly for high-risk groups, expanding screening programs and addressing lifestyle factors (i.e. poor diet and sedentary behaviour) can minimise the impact of rising cancer rates and maximise patient outcomes.
- Investing in cancer research and care, can also reduce the impact of increasing cancer rates and improve outcomes for patients.

## Challenges & Limitations
- Cancer rates are notably high in Asia, Europe, and the Americas, but these rates may not be as high in other continents due to a lack of reliable data from those regions.
- Examining environmental factors, such as air pollution and exposure to carcinogens, could offer further insights into the rising cancer rates.



## Spotify Listeners Analysis

![Spotify Listeners Analysis](https://github.com/abea3costa/Portfolio/blob/main/Tableau%20Dashboards/Spotify%20listeners%20analysis.png)

## Overview
The objective of this project is to create an intuitive dashboard which highlights key aspects regarding Spotify listeners‚Äô trends. 

## Technologies Used
- Tableau: Used different sheets for each visualisation to then combine them in a dashboard. Utilised calculated fields to convert song duration from milliseconds to minutes (Duration in Minutes=Duration in milliseconds/60000).

## Dataset Description
- The dataset includes 232,725 records covering various aspects of Spotify listeners' habits, such as different artists, songs, genres, as well as quantitative measures (acousticness, energy, duration, and tempo) and qualitative attributes (key and mode) of the songs.
- Source: JustIT Data Technician Bootcamp: [Spotify Listeners Data](Extension_Task_SpotifyFeatures.csv)

## Methodology
- Visualisation: The dashboard features several visualizations, including bar charts, a pie chart, a tree map, and two scatter plots, to showcase key insights on the most popular artists and genres, the most versatile artists (spanning different genres), the correlation between song duration and tempo, and the relationship between acousticness and energy.
- User-Friendly Interface: The dashboard is designed with a user-friendly interface, ensuring easy navigation and accessibility.
- Customizable: The dashboard is highly customizable, allowing users to add or modify charts and tables based on specific changes in data.

## Key Findings
- The top 10 artists span a wide range of genres, including Hip-Hop, R&B, Classical, and movie and video game scores.
- The most popular genres are Pop, Rap, Rock, Hip-Hop, Indie, and Children‚Äôs Music.
- Users tend to prefer songs in Major over Minor modes. 
- The artists with the most versatility‚Äîhaving songs across 8 different genres‚Äîinclude The Cocteau Twins, dvsn, SiR, and Toro Y Moi.
- There‚Äôs a wide variety of relationships between song duration and tempo: most genres fall within the 110-125 bpm range, with playtimes between 3.5 and 4 minutes. 
- Acousticness appears to be inversely related to Energy. Artists with high acousticness typically create music that is almost entirely acoustic, with little to no electronic or synthetic elements. In contrast, artists with high energy create upbeat, fast-paced tracks that often feature synthesized sounds, electronic beats, and digital effects, resulting in low acousticness.

## Results & Conclusions
- This data enables personalized recommendations, targeted marketing, and content curation, which can enhance user engagement and retention.
- Understanding trends allows Spotify to optimize its playlists, improve user experience, and inform decisions about partnerships, advertising, and future content investments.

## Challenges & Limitations
- Spotify's algorithm can contribute to users listening to the same music, especially within algorithmic playlists like "Discover Weekly" or "Release Radar," as it prioritizes popular songs and those similar to users‚Äô listening history, leading to personalized but potentially repetitive music. To tackle this, Spotify can introduce:
  - A feature that intentionally includes less popular or unfamiliar tracks alongside familiar ones in these playlists.
  - Allow users to set preferences for how much variety they want in their recommendations. For instance, users could toggle between "familiar" and "exploratory" modes.
  - Introduce suggestions based on diverse listening patterns from users with similar but distinct tastes, encouraging a mix of familiar and novel tracks.



## Gaming Sales

![Gaming Sales Analysis](https://github.com/abea3costa/Portfolio/blob/main/Tableau%20Dashboards/Gaming%20Sales%20Analysis.png)

## Overview
The objective of this project is to create an intuitive dashboard which showcases the most profitable markets in the gaming industry, as well as the evolution of sales and game releases over time. 

## Technologies Used
- Tableau: Used different sheets for each visualisation to then combine them in a dashboard. 

## Dataset Description
- The dataset includes 12,441 records covering information regarding videogame sales, such as, genre, platform and publisher.
- Source: JustIT Data Technician Bootcamp: [Videogame Sales](Day_2_Gaming_Sales.twbx)

## Methodology
- Visualisation: The dashboard features several visualizations, such as line charts, bar charts and a column chart, to showcase the number of video game releases over time, global and european sales trends, and the most profitable video games, genres, and platforms (e.g., PS2, Wii, Nintendo DS, etc.).
- User-Friendly Interface: The dashboard is designed with a user-friendly interface, ensuring easy navigation and accessibility.
- Customizable: The dashboard is highly customizable, allowing users to add or modify charts and tables based on specific changes in data.

## Key Findings
- Between 1980 and 2010, Videogames releases peaked around 2008-2009 with both global and European sales following the same pattern.
- The top 5 most profitable genres are Action, Sports, Platform, Role-Playing and Shooter.
- The top 5 most profitable platforms are PlayStation 2, Wii, Nintendo DS, PlayStation and X360.
- Electronic Arts and Nintendo released the most games in total with marked versatility in genres.
- Best sellers include Wii Sports, Super Mario Bros and Mario Kart Wii, in the category of sports, platform and racing respectively. 

## Results & Conclusions
- Analysing video game trends and patterns helps publishers make informed decisions about their upcoming releases, maximizing profitability.
- Action and Sports games appear to be the most profitable genres, making them a reliable choice for publishers.

## Challenges & Limitations
- The long names of video game publishers and titles can impact the dashboard‚Äôs design, affecting both its visual appeal and readability. Abbreviating names would compromise the clarity of the charts, making them difficult to interpret.



# Power BI
  
## Tech Job Satisfaction Survey

![Tech Job Satisfaction Survey](https://github.com/abea3costa/Portfolio/blob/main/PowerBI%20Dashboards/Professional%20Survey%20Breakdown.png)

## Overview
The main goal of this project is to create an interactive and intuitive dashboard which highlights the main insights derived from the analysis of job satisfaction survey results across various tech careers.

## Technologies Used
- Power BI (Table view, DAX, split columns by delimiter, filters, visualisations formatting and editing)

## Dataset Description
- The dataset consists of 630 records, with each record representing a survey participant. The questionnaire (columns) includes job title, salary bracket, industry, preferred programming language, country, level of education, and ratings (0-10) on factors such as work-life balance, career progression opportunities, management, etc.
- Source: AlexTheAnalyst GitHub: [Job Satisfaction Survey](https://github.com/AlexTheAnalyst/Power-BI/blob/main/Power%20BI%20-%20Final%20Project.xlsx)


## Methodology
- Data Cleaning: Removed duplicates and irrelevant columns for my analysis. Used DAX to split the salary bracket column by delimiters, to make it easier to create a new column with the average salary per tech role. I also used this technique to remove any irrelevant information from free-text boxes that could make the dashboard appear cluttered, consolidating all unique responses to the "Other, please specify" field into a single category labelled "Other‚Äù. Standardised the format and ensured all columns had the appropriate data types.
- Visualisation: The dashboard includes cards, a bar chart, a stacked column chart, two gauges, a tree map, and a donut chart, which showcase key statistics from the tech job satisfaction survey.
- User-Friendly Interface: The dashboard is designed with a user-friendly interface, ensuring easy navigation and accessibility.
- Customizable: The dashboard is highly customizable, allowing users to add or modify charts and tables based on specific changes in data.

## Key Findings
- There was a total of 630 participants in this survey, out of which 50.8% were females. 
- The largest sample by job role were the Data Analysts, with a total of 381 participants.
- The average age of the participants is 29.87 years old.
- The United States corresponded to 41.4% of the survey takers, although there were participants from all over the world.
- Data Scientists present the highest average salaries, followed by Data Engineers and Data Architects.
- Most tech professionals prefer Python for programming.
- The salary happiness rating is 4.27 out of 10.
- The work/life balance rating is 5.74 out of 10.

## Results & Conclusions
- The information presented by the dashboard can be highly valuable for individuals aspiring to pursue a career in data. Based on the factors that matter most to each person‚Äîwhether it‚Äôs salary, work-life balance, or career progression opportunities‚Äîthis data analysis can assist in making more informed decisions about their professional future.
- Data Science could offer a promising career, with the highest average salaries, along with the highest salary satisfaction and work/life balance ratings. However, the number of Data Scientists participating in the survey was significantly smaller compared to the sample of Data Analysts. It would be beneficial to engage more Data Scientists to gather their input and create a broader, more representative sample of survey responses.

## Challenges & Limitations
- This dataset is based on a real survey conducted by Alex the Analyst (YouTube Channel). While it offers valuable data and very useful post-analysis insights, the survey appears to have primarily targeted Data Analysts, which isn‚Äôt surprising given that Alex Freberg‚Äôs followers are mostly Data Analysts or aspiring Data Analysts. The survey's structure is excellent; however, increasing the sample of the other data jobs would enhance the dataset, making it more informative and offering a comprehensive overview of various data roles‚Äîparticularly for those still uncertain about their data career path.



## Apocalypse Preparation Analysis

![Apocalypse Preparation Analysis](https://github.com/abea3costa/Portfolio/blob/main/PowerBI%20Dashboards/Apocalypse%20prep%20Sales.png)

## Overview
This project aims to create an intuitive, interactive dashboard that presents key insights into the sales of apocalypse preparation items across different retail shops.

## Technologies Used
- Power BI (Table view, Model view, DAX, split columns by delimiter, filters, visualisations formatting and editing)

## Dataset Description
- The dataset includes 142 records spread across different tables: one containing inventory data with product prices and production costs, another with sales details, a third with retailer information, and the final one with customer details.
- Source: AlexTheAnalyst GitHub: [Apocalypse Dataset]( https://github.com/AlexTheAnalyst/Power-BI/blob/main/Apocolypse%20Food%20Prep%20-%20Visualizations%20Tutorial.xlsx)

## Methodology
- Managing relationships: On the Model view tab, established the relationships by linking the tables by Customer ID and Product ID.
![Modeling Apocalypse](https://github.com/abea3costa/Portfolio/blob/main/PowerBI%20Dashboards/Data%20Modelling%20Apocalypse.png)
- Data Cleaning: Removed duplicates, used DAX to split columns by delimiters, to make it easier to correct the data type of the price column. Standardized the format and ensured all columns had the appropriate data types.
- Visualisation: The dashboard features cards, a table, a scatter plot, a donut chart, a line chart, one bar chart and two column charts to showcase popular product choices, units sold over time, customer purchases across different US states, and the correlation between price and cost.
- User-Friendly Interface: The dashboard is designed with a user-friendly interface, ensuring easy navigation and accessibility.
- Customizable: The dashboard is highly customizable, allowing users to add or modify charts and tables based on specific changes in data.

## Key Findings
- The multitool survival knife, the nylon rope and the duct tape are the top 3 best sellers.
- The shop with the highest sales was Uncle Joe‚Äôs Prep Shop.
- Out of a total of 22,000 units sold, 6,000 units were sold in the state of Minnesota.
- There was a marked peak of sales for duct tape around the 22nd day of the month. 
- Although the weatherproof jacket has one of the highest production costs, second only to the stainless-steel axe, it is sold for more than 150% of its cost, making it the item with the highest profit margin.
- The duct tape and the N95 mask present the lowest production costs and prices. However, this results in very little profit.

## Results & Conclusions
- The Minnesota appears to be a good option for shops that sell items for the apocalypse.
- The Nylon Rope and the Multitool survival knife both have really good profit margins and sell really well, so they would be a good investment and worth stocking up the inventory for. 
- The duct tape despite its popularity, duct tape is sold at a price nearly equal to its production cost. I would suggest buying it from a cheaper supplier or raising the price. Increasing the price from $6.25 to $6.87 could boost profits by almost 45%. Given the duct tape's popularity, customers are unlikely to notice the 10% price increase. 
- In order to increase the N95 profitability as well, these could be sold as a bundle with the duct tape and an item with a significant profit margin such as the weatherproof jacket or the nylon rope.

## Challenges & Limitations
- Although this is a fictional dataset, it offered valuable experience using Power BI‚Äôs model view, establishing relationships, and extracting meaningful insights related to retail sales and valuable marketing strategy research, all of which are applicable to similar real-life scenarios.



# Azure

## Paws & Whiskers transition to Microsoft Azure

### Overview
This project is an analysis of the Microsoft Azure products most suitable for "Paws and Whiskers", a pet shop aiming to expand its business. The main focus is the optimisation of data storage, analysis, and reporting to help the business make data-driven decisions as it grows.

### Click here to download the pdf: [Paws and Whiskers - A pet shop transition to Microsoft Azure](https://github.com/abea3costa/Portfolio/blob/main/Paws%20and%20Whiskers%20Project.pdf)

  



